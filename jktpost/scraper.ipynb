{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jakarta Post Business News Scraper\n",
    "\n",
    "A Jupyter notebook version of the Jakarta Post business news scraper.\n",
    "\n",
    "## Features\n",
    "- Scrapes articles from the business section\n",
    "- Configurable date range\n",
    "- Optional full content fetching\n",
    "- Duplicate prevention\n",
    "- Saves to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Run this cell to install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests beautifulsoup4 lxml -q\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, List, Dict, Tuple, Set\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Set your scraping parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Days back: 2\n",
      "  Output file: news_data.json\n",
      "  Fetch content: True\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DAYS_BACK = 2  # Number of days back to scrape\n",
    "OUTPUT_FILE = \"news_data.json\"  # Output JSON file\n",
    "BASE_URL = \"https://www.thejakartapost.com/business/latest\"  # Base URL to scrape\n",
    "FETCH_CONTENT = True  # Set to True to fetch full article content (slower)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Days back: {DAYS_BACK}\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")\n",
    "print(f\"  Fetch content: {FETCH_CONTENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scraper Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraper class defined!\n"
     ]
    }
   ],
   "source": [
    "class JakartaPostScraper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        days_back: int = 2,\n",
    "        output_file: str = \"news_data.json\",\n",
    "        base_url: str = \"https://www.thejakartapost.com/business/latest\",\n",
    "        fetch_content: bool = False\n",
    "    ):\n",
    "        self.days_back = days_back\n",
    "        self.output_file = output_file\n",
    "        self.base_url = base_url\n",
    "        self.fetch_content = fetch_content\n",
    "        self.cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "        self.existing_urls = self._load_existing_urls()\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.0'\n",
    "        })\n",
    "        self.article_url_pattern = re.compile(r'/business/\\d{4}/\\d{2}/\\d{2}/')\n",
    "        self.scraped_articles: List[Dict] = []\n",
    "        self.new_articles: List[Dict] = []\n",
    "    \n",
    "    def _load_existing_urls(self) -> Set[str]:\n",
    "        if not os.path.exists(self.output_file):\n",
    "            return set()\n",
    "        try:\n",
    "            with open(self.output_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                return {item['url'] for item in data if 'url' in item}\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            return set()\n",
    "    \n",
    "    def _load_existing_data(self) -> List[Dict]:\n",
    "        if not os.path.exists(self.output_file):\n",
    "            return []\n",
    "        try:\n",
    "            with open(self.output_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "    \n",
    "    def _save_data(self, data: List[Dict]):\n",
    "        with open(self.output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False, default=str)\n",
    "        print(f\"Saved {len(data)} articles to {self.output_file}\")\n",
    "    \n",
    "    def _extract_date_from_url(self, url: str) -> Optional[datetime]:\n",
    "        match = re.search(r'/business/(\\d{4})/(\\d{2})/(\\d{2})/', url)\n",
    "        if match:\n",
    "            year, month, day = match.groups()\n",
    "            try:\n",
    "                return datetime(int(year), int(month), int(day))\n",
    "            except ValueError:\n",
    "                return None\n",
    "        return None\n",
    "    \n",
    "    def _fetch_page(self, page_num: int) -> Optional[BeautifulSoup]:\n",
    "        url = f\"{self.base_url}?page={page_num}\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching page {page_num}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _fetch_article_content(self, url: str) -> Dict:\n",
    "        content_data = {\n",
    "            'content': '',\n",
    "            'author': '',\n",
    "            'category': '',\n",
    "            'tags': [],\n",
    "            'fetch_error': None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract author\n",
    "            author_elem = soup.find('meta', attrs={'name': 'author'})\n",
    "            if author_elem:\n",
    "                content_data['author'] = str(author_elem.get('content', '')).strip()\n",
    "            else:\n",
    "                author_elem = soup.find('span', class_='tjp-meta__label')\n",
    "                if author_elem:\n",
    "                    content_data['author'] = author_elem.get_text(strip=True)\n",
    "            \n",
    "            # Extract category\n",
    "            category_elem = soup.find('li', class_='tjp-breadcrumb__list-item active')\n",
    "            if category_elem:\n",
    "                content_data['category'] = category_elem.get_text(strip=True)\n",
    "            \n",
    "            # Extract content\n",
    "            content_elem = soup.select_one('div.tjp-single__content')\n",
    "            if content_elem:\n",
    "                paragraphs = content_elem.find_all('p')\n",
    "                if paragraphs:\n",
    "                    content_text = '\\n\\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "                    if len(content_text) > 50:\n",
    "                        content_data['content'] = content_text\n",
    "            \n",
    "            # Fallback\n",
    "            if not content_data['content']:\n",
    "                article_area = soup.find('div', class_='tjp-single') or soup.find('article')\n",
    "                if article_area:\n",
    "                    paragraphs = article_area.find_all('p')\n",
    "                    content_text = '\\n\\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "                    if len(content_text) > 50:\n",
    "                        content_data['content'] = content_text\n",
    "            \n",
    "            # Extract tags\n",
    "            keywords_elem = soup.find('meta', attrs={'name': 'keywords'})\n",
    "            if keywords_elem:\n",
    "                keywords = keywords_elem.get('content', '')\n",
    "                if keywords:\n",
    "                    content_data['tags'] = [tag.strip() for tag in str(keywords).split(',') if tag.strip()]\n",
    "            \n",
    "        except Exception as e:\n",
    "            content_data['fetch_error'] = str(e)\n",
    "        \n",
    "        return content_data\n",
    "    \n",
    "    def _extract_article_data(self, article_elem, skip_existing: bool = True) -> Optional[Dict]:\n",
    "        try:\n",
    "            link_elem = article_elem.find('a', href=True)\n",
    "            if not link_elem:\n",
    "                return None\n",
    "            \n",
    "            url = urljoin(\"https://www.thejakartapost.com\", link_elem['href'])\n",
    "            \n",
    "            if not self.article_url_pattern.search(url):\n",
    "                return None\n",
    "            \n",
    "            is_existing = url in self.existing_urls\n",
    "            if is_existing and skip_existing:\n",
    "                return None\n",
    "            \n",
    "            title_elem = article_elem.find('h2', class_='titleNews')\n",
    "            if not title_elem:\n",
    "                title_elem = article_elem.find(['h2', 'h3', 'h1'])\n",
    "            if not title_elem:\n",
    "                title_elem = link_elem\n",
    "            \n",
    "            title = title_elem.get_text(strip=True) if title_elem else \"No title\"\n",
    "            \n",
    "            article_date = self._extract_date_from_url(url)\n",
    "            date_str = None\n",
    "            \n",
    "            date_elem = article_elem.find('span', class_='date')\n",
    "            if date_elem:\n",
    "                date_text = date_elem.get_text(strip=True)\n",
    "                parsed_date = self._try_parse_date(date_text)\n",
    "                if parsed_date:\n",
    "                    article_date = parsed_date\n",
    "            \n",
    "            summary_elem = article_elem.find('p')\n",
    "            summary = summary_elem.get_text(strip=True) if summary_elem else \"\"\n",
    "            \n",
    "            return {\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'date': article_date.isoformat() if article_date else date_str,\n",
    "                'date_parsed': article_date,\n",
    "                'summary': summary,\n",
    "                'is_existing': is_existing,\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _try_parse_date(self, date_str: str) -> Optional[datetime]:\n",
    "        date_formats = [\n",
    "            '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%S%z',\n",
    "            '%Y-%m-%d %H:%M:%S', '%B %d, %Y', '%b %d, %Y',\n",
    "            '%d %B %Y', '%d %b %Y', '%Y-%m-%d',\n",
    "        ]\n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                return datetime.strptime(date_str.strip(), fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return None\n",
    "    \n",
    "    def _find_articles_on_page(self, soup: BeautifulSoup) -> Tuple[List[Dict], List[Dict]]:\n",
    "        all_articles = []\n",
    "        new_articles = []\n",
    "        processed_urls = set()\n",
    "        \n",
    "        list_news = soup.find_all('div', class_='listNews')\n",
    "        \n",
    "        for elem in list_news:\n",
    "            article_data = self._extract_article_data(elem, skip_existing=False)\n",
    "            if article_data and article_data['url'] not in processed_urls:\n",
    "                all_articles.append(article_data)\n",
    "                processed_urls.add(article_data['url'])\n",
    "                if not article_data['is_existing']:\n",
    "                    new_articles.append(article_data)\n",
    "        \n",
    "        if not all_articles:\n",
    "            all_links = soup.find_all('a', href=True)\n",
    "            for link in all_links:\n",
    "                href = str(link.get('href', ''))\n",
    "                if self.article_url_pattern.search(href):\n",
    "                    parent = link.find_parent(['div', 'article', 'li', 'section'])\n",
    "                    if parent:\n",
    "                        article_data = self._extract_article_data(parent, skip_existing=False)\n",
    "                        if article_data and article_data['url'] not in processed_urls:\n",
    "                            all_articles.append(article_data)\n",
    "                            processed_urls.add(article_data['url'])\n",
    "                            if not article_data['is_existing']:\n",
    "                                new_articles.append(article_data)\n",
    "        \n",
    "        return all_articles, new_articles\n",
    "    \n",
    "    def _should_stop_pagination(self, all_articles: List[Dict], page_num: int) -> bool:\n",
    "        if not all_articles:\n",
    "            print(f\"  No articles found on page {page_num}, stopping\")\n",
    "            return True\n",
    "        \n",
    "        dated_articles = [a for a in all_articles if a.get('date_parsed')]\n",
    "        \n",
    "        if dated_articles and all(a['date_parsed'] < self.cutoff_date for a in dated_articles):\n",
    "            print(f\"  All articles on page {page_num} are older than cutoff\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def scrape(self) -> List[Dict]:\n",
    "        print(f\"\\nStarting scrape with {self.days_back} days back (cutoff: {self.cutoff_date.date()})\\n\")\n",
    "        \n",
    "        all_articles = self._load_existing_data()\n",
    "        new_articles = []\n",
    "        page_num = 1\n",
    "        stop_pagination = False\n",
    "        \n",
    "        while not stop_pagination:\n",
    "            print(f\"Fetching page {page_num}...\")\n",
    "            soup = self._fetch_page(page_num)\n",
    "            if not soup:\n",
    "                print(f\"  Failed to fetch page {page_num}\")\n",
    "                break\n",
    "            \n",
    "            page_all_articles, page_new_articles = self._find_articles_on_page(soup)\n",
    "            \n",
    "            print(f\"  Found {len(page_all_articles)} total ({len(page_new_articles)} new) articles\")\n",
    "            \n",
    "            for article in page_new_articles:\n",
    "                new_articles.append(article)\n",
    "                self.existing_urls.add(article['url'])\n",
    "            \n",
    "            if self._should_stop_pagination(page_all_articles, page_num):\n",
    "                stop_pagination = True\n",
    "            \n",
    "            page_num += 1\n",
    "            if page_num > 100:\n",
    "                print(\"  Reached maximum page limit (100)\")\n",
    "                break\n",
    "        \n",
    "        if new_articles:\n",
    "            if self.fetch_content:\n",
    "                print(f\"\\nFetching content for {len(new_articles)} new articles...\\n\")\n",
    "                for i, article in enumerate(new_articles, 1):\n",
    "                    print(f\"  [{i}/{len(new_articles)}] {article['title'][:50]}...\")\n",
    "                    content_data = self._fetch_article_content(article['url'])\n",
    "                    article['content'] = content_data['content']\n",
    "                    article['author'] = content_data['author']\n",
    "                    article['category'] = content_data['category']\n",
    "                    article['tags'] = content_data['tags']\n",
    "                    if content_data['fetch_error']:\n",
    "                        article['content_error'] = content_data['fetch_error']\n",
    "            \n",
    "            all_articles.extend(new_articles)\n",
    "            self._save_data(all_articles)\n",
    "            print(f\"\\nAdded {len(new_articles)} new articles\")\n",
    "        else:\n",
    "            print(\"\\nNo new articles found\")\n",
    "        \n",
    "        self.scraped_articles = all_articles\n",
    "        self.new_articles = new_articles\n",
    "        return new_articles\n",
    "\n",
    "print(\"Scraper class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Scraper\n",
    "\n",
    "Execute the scraping process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting scrape with 2 days back (cutoff: 2026-02-16)\n",
      "\n",
      "Fetching page 1...\n",
      "  Found 28 total (28 new) articles\n",
      "Fetching page 2...\n",
      "  Found 28 total (28 new) articles\n",
      "  All articles on page 2 are older than cutoff\n",
      "\n",
      "Fetching content for 56 new articles...\n",
      "\n",
      "  [1/56] Bumi Resources Minerals says Palu operations unaff...\n",
      "  [2/56] New state miner Perminas signs MoU on Gabon rare e...\n",
      "  [3/56] As Global Attention Turns to the US: What Are Its ...\n",
      "  [4/56] Coal must go for climate funds to flow, German amb...\n",
      "  [5/56] Trump says Japan to invest in energy, industrial p...\n",
      "  [6/56] Govt urged not to compromise too much for US trade...\n",
      "  [7/56] Unlocking regional potential to boost economic gro...\n",
      "  [8/56] Job threats, rogue bots: five hot issues in AI...\n",
      "  [9/56] Dollar holds gains in thin trading as markets awai...\n",
      "  [10/56] Idle oil and gas projects risk losing licenses, Ba...\n",
      "  [11/56] Site selection for first nuclear plant expected by...\n",
      "  [12/56] Asian markets sluggish as Lunar New Year holiday l...\n",
      "  [13/56] Japan's economy limps back to scant growth in Q4, ...\n",
      "  [14/56] Businesses seek AI regulatory clarity as caution h...\n",
      "  [15/56] Vietnam licenses Musk's satellite internet firm St...\n",
      "  [16/56] China’s ByteDance releases Doubao 2.0 AI model for...\n",
      "  [17/56] EU proposes extending Russian oil sanctions to mar...\n",
      "  [18/56] Lunar New Year drives corporate decor boom, but re...\n",
      "  [19/56] 'Be realistic' says Prabowo, while defending key p...\n",
      "  [20/56] Tiffany & Co stores sealed over alleged illegal im...\n",
      "  [21/56] High-rise housing still a hard sell in Jakarta, sa...\n",
      "  [22/56] EU leaders vow to accelerate single market, in str...\n",
      "  [23/56] Asian shares step back from record as tech jitters...\n",
      "  [24/56] Danantara’s poultry push raises concerns over mark...\n",
      "  [25/56] Pertamina to spin off airlines, hospitals, non-oil...\n",
      "  [26/56] Prabowo ordered purge of IDX, OJK leadership, brot...\n",
      "  [27/56] Gold edges lower on firmer dollar after strong US ...\n",
      "  [28/56] Danantara’s IDX ownership plan gets guarded welcom...\n",
      "  [29/56] Indonesia ready to bridge trade, investment in Isl...\n",
      "  [30/56] Indosat Ooredoo Hutchison shows healthy fundamenta...\n",
      "  [31/56] PLN EPI breaks ground on Rp1 trillion gas pipeline...\n",
      "  [32/56] SWA RoboKnights to represent Indonesia at VEX Robo...\n",
      "  [33/56] Buoyant stocks pause for breath on signs of soften...\n",
      "  [34/56] Pertamina denies Russian crude imports as EU mulls...\n",
      "  [35/56] Retail sales post strong annual gain as consumer c...\n",
      "  [36/56] Finance Ministry’s SMI secures $9.3m grants to exp...\n",
      "  [37/56] Indonesia’s growth: Resilient but uneven...\n",
      "  [38/56] Fiscal discipline remains intact, govt insists, af...\n",
      "  [39/56] Jury told that Meta, Google 'engineered addiction'...\n",
      "  [40/56] Bulog proposes Mecca warehouse to stock haj rice e...\n",
      "  [41/56] EU proposes sanctions on Indonesian ports for hand...\n",
      "  [42/56] FTSE Russell postpones Indonesia index review...\n",
      "  [43/56] Poverty at record low as govt seeks to redefine th...\n",
      "  [44/56] Indonesia proposes 3 measures to meet MSCI's trans...\n",
      "  [45/56] Thomas Djiwandono sworn in as BI deputy governor...\n",
      "  [46/56] Danantara breaks ground on $7b downstream industry...\n",
      "  [47/56] Japan's Takaichi may struggle to soothe voters and...\n",
      "  [48/56] Asian markets rally as Japan shares surge to recor...\n",
      "  [49/56] Crackdown on mining permits faces legal scrutiny...\n",
      "  [50/56] Danantara steps in as primary funder for new Kraka...\n",
      "  [51/56] EU tells TikTok to change 'addictive' design...\n",
      "  [52/56] Crypto firm accidentally sends $40 billion in bitc...\n",
      "  [53/56] Unemployment figure subsides despite growing layof...\n",
      "  [54/56] Economic fundamentals solid, govt insists, after M...\n",
      "  [55/56] Ex-BI deputy replaces Tommy as deputy finance mini...\n",
      "  [56/56] Indonesian markets slide again after Moody's cuts ...\n",
      "Saved 56 articles to news_data.json\n",
      "\n",
      "Added 56 new articles\n",
      "\n",
      "============================================================\n",
      "SCRAPING COMPLETE!\n",
      "============================================================\n",
      "New articles added: 56\n",
      "Total articles in file: 56\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run scraper\n",
    "scraper = JakartaPostScraper(\n",
    "    days_back=DAYS_BACK,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    base_url=BASE_URL,\n",
    "    fetch_content=FETCH_CONTENT\n",
    ")\n",
    "\n",
    "# Run the scrape\n",
    "new_articles = scraper.scrape()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SCRAPING COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"New articles added: {len(new_articles)}\")\n",
    "print(f\"Total articles in file: {len(scraper.scraped_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Results\n",
    "\n",
    "Display the scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCRAPING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total new articles: 56\n",
      "Total articles in database: 56\n",
      "\n",
      "Articles by date:\n",
      "  2026-02-06: 3 articles\n",
      "  2026-02-07: 2 articles\n",
      "  2026-02-08: 3 articles\n",
      "  2026-02-09: 6 articles\n",
      "  2026-02-10: 9 articles\n",
      "  2026-02-11: 5 articles\n",
      "  2026-02-12: 5 articles\n",
      "  2026-02-13: 5 articles\n",
      "  2026-02-14: 2 articles\n",
      "  2026-02-15: 3 articles\n",
      "  2026-02-16: 4 articles\n",
      "  2026-02-17: 4 articles\n",
      "  2026-02-18: 5 articles\n",
      "\n",
      "Content stats:\n",
      "  Articles with content: 56/56\n",
      "  Total content chars: 157,040\n",
      "  Avg content length: 2,804 chars\n"
     ]
    }
   ],
   "source": [
    "# Display summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SCRAPING SUMMARY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"Total new articles: {len(new_articles)}\")\n",
    "print(f\"Total articles in database: {len(scraper.scraped_articles)}\")\n",
    "\n",
    "if new_articles:\n",
    "    # Show date distribution\n",
    "    from collections import Counter\n",
    "    dates = [a.get('date', 'unknown')[:10] for a in new_articles]\n",
    "    date_counts = Counter(dates)\n",
    "    \n",
    "    print(f\"\\nArticles by date:\")\n",
    "    for date in sorted(date_counts.keys()):\n",
    "        print(f\"  {date}: {date_counts[date]} articles\")\n",
    "    \n",
    "    # Content stats if available\n",
    "    if FETCH_CONTENT:\n",
    "        with_content = sum(1 for a in new_articles if a.get('content'))\n",
    "        total_chars = sum(len(a.get('content', '')) for a in new_articles)\n",
    "        print(f\"\\nContent stats:\")\n",
    "        print(f\"  Articles with content: {with_content}/{len(new_articles)}\")\n",
    "        print(f\"  Total content chars: {total_chars:,}\")\n",
    "        print(f\"  Avg content length: {total_chars // len(new_articles) if new_articles else 0:,} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAMPLE ARTICLE\n",
      "============================================================\n",
      "\n",
      "title: Bumi Resources Minerals says Palu operations unaffected by site closure\n",
      "url: https://www.thejakartapost.com/business/2026/02/18/bumi-resources-minerals-says-palu-operations-unaf...\n",
      "date: 2026-02-18T00:00:00\n",
      "date_parsed: 2026-02-18 00:00:00\n",
      "summary: The publicly listed miner, while confirming the closure of a site within subsidiary CPM's forest con...\n",
      "is_existing: False\n",
      "scraped_at: 2026-02-18T09:20:55.717217\n",
      "content:\n",
      "  T Bumi Resources Minerals (BRM) says the government's closure of a contested gold mine at its concession in Palu, Central Sulawesi, will not disrupt core production activities, as the site in question was not yet operational.\n",
      "\n",
      "The publicly listed miner said in a statement on Monday that the forest area enforcement task force (Satgas PKH) had sealed off a location within a contract of work area managed by its subsidiary, Citra Palu Minerals (CPM).\n",
      "\n",
      "The Satgas PKH team moved to seal the site after...\n",
      "author: Divya Karyza\n",
      "category: Companies\n",
      "tags: ['BRMS', 'mining', 'PKH']\n"
     ]
    }
   ],
   "source": [
    "# Display sample article\n",
    "if new_articles:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAMPLE ARTICLE\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    sample = new_articles[0]\n",
    "    \n",
    "    for key, value in sample.items():\n",
    "        if key == 'content' and value:\n",
    "            print(f\"{key}:\")\n",
    "            print(f\"  {value[:500]}...\" if len(value) > 500 else f\"  {value}\")\n",
    "        elif isinstance(value, list):\n",
    "            print(f\"{key}: {value}\")\n",
    "        elif isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"{key}: {value[:100]}...\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Export & Analysis\n",
    "\n",
    "Export or analyze the scraped data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame shape: (56, 11)\n",
      "\n",
      "Columns: ['title', 'url', 'date', 'date_parsed', 'summary', 'is_existing', 'scraped_at', 'content', 'author', 'category', 'tags']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>date_parsed</th>\n",
       "      <th>summary</th>\n",
       "      <th>is_existing</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bumi Resources Minerals says Palu operations u...</td>\n",
       "      <td>https://www.thejakartapost.com/business/2026/0...</td>\n",
       "      <td>2026-02-18T00:00:00</td>\n",
       "      <td>2026-02-18 00:00:00</td>\n",
       "      <td>The publicly listed miner, while confirming th...</td>\n",
       "      <td>False</td>\n",
       "      <td>2026-02-18T09:20:55.717217</td>\n",
       "      <td>T Bumi Resources Minerals (BRM) says the gover...</td>\n",
       "      <td>Divya Karyza</td>\n",
       "      <td>Companies</td>\n",
       "      <td>[BRMS, mining, PKH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New state miner Perminas signs MoU on Gabon ra...</td>\n",
       "      <td>https://www.thejakartapost.com/business/2026/0...</td>\n",
       "      <td>2026-02-18T00:00:00</td>\n",
       "      <td>2026-02-18 00:00:00</td>\n",
       "      <td>PT Perusahaan Mineral Nasional and Danantara m...</td>\n",
       "      <td>False</td>\n",
       "      <td>2026-02-18T09:20:55.717752</td>\n",
       "      <td>he country’s newly established state-owned min...</td>\n",
       "      <td>Ruth Dea Juwita</td>\n",
       "      <td>Companies</td>\n",
       "      <td>[Perminas, Danantara, rare-earths, investment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As Global Attention Turns to the US: What Are ...</td>\n",
       "      <td>https://www.thejakartapost.com/business/2026/0...</td>\n",
       "      <td>2026-02-18T00:00:00</td>\n",
       "      <td>2026-02-18 00:00:00</td>\n",
       "      <td>With much of the global conversation currently...</td>\n",
       "      <td>False</td>\n",
       "      <td>2026-02-18T09:20:55.718037</td>\n",
       "      <td>ith much of the global conversation currently ...</td>\n",
       "      <td>Creative Desk</td>\n",
       "      <td>Companies</td>\n",
       "      <td>[adv-BC, adv-usaseanbusinesscouncil, AWS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coal must go for climate funds to flow, German...</td>\n",
       "      <td>https://www.thejakartapost.com/business/2026/0...</td>\n",
       "      <td>2026-02-18T00:00:00</td>\n",
       "      <td>2026-02-18 00:00:00</td>\n",
       "      <td>Private funds could flow elsewhere without coa...</td>\n",
       "      <td>False</td>\n",
       "      <td>2026-02-18T09:20:55.718265</td>\n",
       "      <td>ermany says billions of dollars in private cli...</td>\n",
       "      <td>Ruth Dea Juwita</td>\n",
       "      <td>Regulations</td>\n",
       "      <td>[Germany, JETP, energy-transition, coal-power-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump says Japan to invest in energy, industri...</td>\n",
       "      <td>https://www.thejakartapost.com/business/2026/0...</td>\n",
       "      <td>2026-02-18T00:00:00</td>\n",
       "      <td>2026-02-18 00:00:00</td>\n",
       "      <td>United States President Donald Trump's adminis...</td>\n",
       "      <td>False</td>\n",
       "      <td>2026-02-18T09:20:55.718472</td>\n",
       "      <td>nited States President Donald Trump's administ...</td>\n",
       "      <td>David Lawder and Jarrett Renshaw</td>\n",
       "      <td>Economy</td>\n",
       "      <td>[US-Japan, trade-agreement, investment, oil-an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Bumi Resources Minerals says Palu operations u...   \n",
       "1  New state miner Perminas signs MoU on Gabon ra...   \n",
       "2  As Global Attention Turns to the US: What Are ...   \n",
       "3  Coal must go for climate funds to flow, German...   \n",
       "4  Trump says Japan to invest in energy, industri...   \n",
       "\n",
       "                                                 url                 date  \\\n",
       "0  https://www.thejakartapost.com/business/2026/0...  2026-02-18T00:00:00   \n",
       "1  https://www.thejakartapost.com/business/2026/0...  2026-02-18T00:00:00   \n",
       "2  https://www.thejakartapost.com/business/2026/0...  2026-02-18T00:00:00   \n",
       "3  https://www.thejakartapost.com/business/2026/0...  2026-02-18T00:00:00   \n",
       "4  https://www.thejakartapost.com/business/2026/0...  2026-02-18T00:00:00   \n",
       "\n",
       "           date_parsed                                            summary  \\\n",
       "0  2026-02-18 00:00:00  The publicly listed miner, while confirming th...   \n",
       "1  2026-02-18 00:00:00  PT Perusahaan Mineral Nasional and Danantara m...   \n",
       "2  2026-02-18 00:00:00  With much of the global conversation currently...   \n",
       "3  2026-02-18 00:00:00  Private funds could flow elsewhere without coa...   \n",
       "4  2026-02-18 00:00:00  United States President Donald Trump's adminis...   \n",
       "\n",
       "   is_existing                  scraped_at  \\\n",
       "0        False  2026-02-18T09:20:55.717217   \n",
       "1        False  2026-02-18T09:20:55.717752   \n",
       "2        False  2026-02-18T09:20:55.718037   \n",
       "3        False  2026-02-18T09:20:55.718265   \n",
       "4        False  2026-02-18T09:20:55.718472   \n",
       "\n",
       "                                             content  \\\n",
       "0  T Bumi Resources Minerals (BRM) says the gover...   \n",
       "1  he country’s newly established state-owned min...   \n",
       "2  ith much of the global conversation currently ...   \n",
       "3  ermany says billions of dollars in private cli...   \n",
       "4  nited States President Donald Trump's administ...   \n",
       "\n",
       "                             author     category  \\\n",
       "0                      Divya Karyza    Companies   \n",
       "1                   Ruth Dea Juwita    Companies   \n",
       "2                     Creative Desk    Companies   \n",
       "3                   Ruth Dea Juwita  Regulations   \n",
       "4  David Lawder and Jarrett Renshaw      Economy   \n",
       "\n",
       "                                                tags  \n",
       "0                                [BRMS, mining, PKH]  \n",
       "1     [Perminas, Danantara, rare-earths, investment]  \n",
       "2          [adv-BC, adv-usaseanbusinesscouncil, AWS]  \n",
       "3  [Germany, JETP, energy-transition, coal-power-...  \n",
       "4  [US-Japan, trade-agreement, investment, oil-an...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display all data\n",
    "import pandas as pd\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        all_data = json.load(f)\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\\n\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No data file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to news_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV (optional)\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    csv_file = OUTPUT_FILE.replace('.json', '.csv')\n",
    "    df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "    print(f\"Exported to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Exploration\n",
    "\n",
    "Search and filter the scraped articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a keyword in the 'keyword' variable above to search\n"
     ]
    }
   ],
   "source": [
    "# Search articles by keyword\n",
    "keyword = \"\"  # Enter your search term here\n",
    "\n",
    "if keyword and new_articles:\n",
    "    matches = [\n",
    "        a for a in new_articles \n",
    "        if keyword.lower() in a.get('title', '').lower() \n",
    "        or keyword.lower() in a.get('summary', '').lower()\n",
    "        or (FETCH_CONTENT and keyword.lower() in a.get('content', '').lower())\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(matches)} articles matching '{keyword}':\\n\")\n",
    "    for i, article in enumerate(matches[:5], 1):\n",
    "        print(f\"{i}. {article['title']}\")\n",
    "        print(f\"   Date: {article.get('date', 'N/A')[:10]}\")\n",
    "        print(f\"   URL: {article['url']}\\n\")\n",
    "else:\n",
    "    print(\"Enter a keyword in the 'keyword' variable above to search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
