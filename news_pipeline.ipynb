{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indonesia Local News Negative Event Detection System\n",
    "\n",
    "\"\"\"\n",
    "Complete pipeline to:\n",
    "1. Fetch RSS feed from Antara News\n",
    "2. Scrape full article content\n",
    "3. Detect negative news using keywords and sentiment analysis\n",
    "4. Store enriched structured data\n",
    "\"\"\"\n",
    "\n",
    "# Cell 1: Import Libraries\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, date\n",
    "from dateutil import parser as date_parser\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094a657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: Configuration Variables\n",
    "\n",
    "RSS_URL = \"https://en.antaranews.com/rss/news\"\n",
    "DATA_DIR = \"data\"\n",
    "LOGS_DIR = \"logs\"\n",
    "REQUEST_DELAY = 2\n",
    "MAX_RETRIES = 3\n",
    "TODAY = date.today().isoformat()\n",
    "\n",
    "NEGATIVE_KEYWORDS = {\n",
    "    'natural_disaster': [\n",
    "        'earthquake', 'flood', 'tsunami', 'volcano', 'eruption', 'landslide',\n",
    "        'avalanche', 'drought', 'storm', 'hurricane', 'typhoon', 'tornado', 'rain'\n",
    "    ],\n",
    "    'crime': [\n",
    "        'arrested', 'arrest', 'murder', 'killed', 'death', 'dead', 'corruption',\n",
    "        'bribery', 'fraud', 'theft', 'stolen', 'robbery', 'assault', 'violence',\n",
    "        'kidnapping', 'drug', 'trafficking', 'smuggling'\n",
    "    ],\n",
    "    'accident': [\n",
    "        'crash', 'collision', 'explosion', 'fire', 'burned', 'injured',\n",
    "        'wounded', 'casualties', 'wreck', 'derail', 'sinking'\n",
    "    ],\n",
    "    'health': [\n",
    "        'outbreak', 'epidemic', 'pandemic', 'virus', 'disease', 'infection',\n",
    "        'contamination', 'poisoning', 'fatal', 'died'\n",
    "    ],\n",
    "    'conflict': [\n",
    "        'protest', 'riot', 'clash', 'fighting', 'attack', 'bombing', 'terrorist',\n",
    "        'hostage', 'siege', 'shooting', 'gunfire', 'unrest'\n",
    "    ],\n",
    "    'economic_distress': [\n",
    "        'bankruptcy', 'bankrupt', 'inflation', 'recession', 'crisis', 'collapse',\n",
    "        'unemployment', 'layoff', 'default', 'debt', 'losses', 'plunge'\n",
    "    ]\n",
    "}\n",
    "\n",
    "ALL_NEGATIVE_KEYWORDS = [\n",
    "    keyword for category in NEGATIVE_KEYWORDS.values() for keyword in category\n",
    "]\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded. Today: {TODAY}\")\n",
    "print(f\"Total negative keywords: {len(ALL_NEGATIVE_KEYWORDS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61565031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Today: 2026-02-17\n",
      "Total negative keywords: 75\n",
      "============================================================\n",
      "INDONESIA LOCAL NEWS NEGATIVE EVENT DETECTION SYSTEM\n",
      "============================================================\n",
      "Pipeline started at: 2026-02-17T15:42:56.725870\n",
      "\n",
      "STEP 1: RSS Ingestion\n",
      "----------------------------------------\n",
      "Fetching RSS feed from: https://en.antaranews.com/rss/news\n",
      "Total articles fetched: 50\n",
      "Articles from today (2026-02-17): 18\n",
      "\n",
      "STEP 2: Article Scraping\n",
      "----------------------------------------\n",
      "Scraping 18 articles...\n",
      "[1/18] Scraping: Village cooperatives not to kill off local small businesses:...\n",
      "[2/18] Scraping: FGM elimination part of human development investment: minist...\n",
      "[3/18] Scraping: Indonesia, Malaysia expand research cooperation on food secu...\n",
      "[4/18] Scraping: Minister urges anticipating extreme weather during Eid trave...\n",
      "[5/18] Scraping: BGN narrows Bali free meals scope during Chinese New Year br...\n",
      "[6/18] Scraping: Indonesia prioritizes elderly pilgrims' protection in 2026 H...\n",
      "[7/18] Scraping: RI explores digital payment technology cooperation with Viet...\n",
      "[8/18] Scraping: Indonesia seeks seamless digital link with Saudi Arabia for ...\n",
      "[9/18] Scraping: Indonesia advances Nusa Penida as integrated green island de...\n",
      "[10/18] Scraping: Prabowo to represent developing nations at BoP first meeting...\n",
      "[11/18] Scraping: Indonesia targets stronger seafood exports with 2026 strateg...\n",
      "[12/18] Scraping: Nirmala Nusantara Rice Cake Festival celebrates Soloâ€™s 281st...\n",
      "[13/18] Scraping: Police investigate theft of Thai tourists' suitcases at Moun...\n",
      "[14/18] Scraping: Chinese New Year: Indonesia grants sentence cuts to 44 inmat...\n",
      "[15/18] Scraping: Gibran sends Chinese New Year wishes, lauds Confucian contri...\n",
      "[16/18] Scraping: Meulang tradition marks start of Holy Ramadan in Meulaboh...\n",
      "[17/18] Scraping: Minister calls free meals program investment in human capita...\n",
      "[18/18] Scraping: Sultan bin Ahmed Visits Shanghai Film Studios, Film Park and...\n",
      "Successfully scraped: 18/18\n",
      "\n",
      "STEP 3: Text Cleaning\n",
      "----------------------------------------\n",
      "Cleaning 18 articles...\n",
      "Text cleaning completed!\n",
      "\n",
      "STEP 4: Negative News Detection\n",
      "----------------------------------------\n",
      "Analyzing 18 articles for negative content...\n",
      "Analysis complete: 0/18 articles marked as negative\n",
      "\n",
      "STEP 5: Storage\n",
      "----------------------------------------\n",
      "Saved 18 articles to: data/2026-02-17_antaranews.json\n",
      "\n",
      "STEP 6: Error Logging\n",
      "----------------------------------------\n",
      "No errors to log.\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETED\n",
      "============================================================\n",
      "Total articles processed: 18\n",
      "Negative articles detected: 0\n",
      "Output file: data/2026-02-17_antaranews.json\n",
      "Completed at: 2026-02-17T15:43:57.593457\n",
      "\n",
      "============================================================\n",
      "ARTICLE SUMMARY\n",
      "============================================================\n",
      "\n",
      "No negative articles detected today.\n",
      "\n",
      "STATISTICS\n",
      "----------------------------------------\n",
      "Total Articles: 18\n",
      "Negative Articles: 0\n",
      "Negative Percentage: 0.0%\n",
      "Avg Negative Score: 0.06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cell 3: RSS Ingestion Agent\n",
    "class RSSIngestionAgent:\n",
    "    def __init__(self, rss_url: str):\n",
    "        self.rss_url = rss_url\n",
    "    \n",
    "    def fetch_feed(self) -> List[Dict]:\n",
    "        print(f\"Fetching RSS feed from: {self.rss_url}\")\n",
    "        feed = feedparser.parse(self.rss_url)\n",
    "        \n",
    "        articles = []\n",
    "        for entry in feed.entries:\n",
    "            article = {\n",
    "                'title': entry.get('title', ''),\n",
    "                'url': entry.get('link', ''),\n",
    "                'published_date': self._parse_date(entry.get('published', '')),\n",
    "                'summary': entry.get('summary', '')\n",
    "            }\n",
    "            articles.append(article)\n",
    "        \n",
    "        print(f\"Total articles fetched: {len(articles)}\")\n",
    "        return articles\n",
    "    \n",
    "    def _parse_date(self, date_str: str) -> str:\n",
    "        try:\n",
    "            parsed = date_parser.parse(date_str)\n",
    "            return parsed.date().isoformat()\n",
    "        except:\n",
    "            return ''\n",
    "    \n",
    "    def filter_today_articles(self, articles: List[Dict]) -> List[Dict]:\n",
    "        today_str = date.today().isoformat()\n",
    "        filtered = [\n",
    "            article for article in articles\n",
    "            if article['published_date'] == today_str\n",
    "        ]\n",
    "        print(f\"Articles from today ({today_str}): {len(filtered)}\")\n",
    "        return filtered\n",
    "\n",
    "# Cell 4: Article Scraper Agent\n",
    "class ArticleScraperAgent:\n",
    "    def __init__(self, delay: int = 2, max_retries: int = 3):\n",
    "        self.delay = delay\n",
    "        self.max_retries = max_retries\n",
    "        self.failed_urls = []\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "    \n",
    "    def scrape_article(self, url: str) -> Optional[str]:\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                time.sleep(self.delay)\n",
    "                response = requests.get(url, headers=self.headers, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                content = self._extract_content(soup)\n",
    "                \n",
    "                if content:\n",
    "                    return content\n",
    "                else:\n",
    "                    print(f\"  Warning: No content found for {url}\")\n",
    "                    return None\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  Attempt {attempt + 1} failed for {url}: {e}\")\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    self.failed_urls.append((url, str(e)))\n",
    "                    return None\n",
    "                time.sleep(self.delay * 2)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _extract_content(self, soup: BeautifulSoup) -> str:\n",
    "        selectors = [\n",
    "            'article', '.post-content', '.entry-content', '.article-content',\n",
    "            '.content', '[itemprop=\"articleBody\"]', '.post', '.news-content'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            content_div = soup.select_one(selector)\n",
    "            if content_div:\n",
    "                paragraphs = content_div.find_all('p')\n",
    "                if paragraphs:\n",
    "                    text = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "                    if len(text) > 100:\n",
    "                        return text\n",
    "        \n",
    "        body = soup.find('body')\n",
    "        if body:\n",
    "            paragraphs = body.find_all('p')\n",
    "            text = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "            return text\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    def scrape_articles(self, articles: List[Dict]) -> List[Dict]:\n",
    "        print(f\"Scraping {len(articles)} articles...\")\n",
    "        enriched_articles = []\n",
    "        \n",
    "        for i, article in enumerate(articles, 1):\n",
    "            print(f\"[{i}/{len(articles)}] Scraping: {article['title'][:60]}...\")\n",
    "            content = self.scrape_article(article['url'])\n",
    "            \n",
    "            if content:\n",
    "                article['content'] = content\n",
    "                enriched_articles.append(article)\n",
    "        \n",
    "        print(f\"Successfully scraped: {len(enriched_articles)}/{len(articles)}\")\n",
    "        return enriched_articles\n",
    "\n",
    "# Cell 5: Text Cleaning Agent\n",
    "class TextCleaningAgent:\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return ''\n",
    "        \n",
    "        text = re.sub(r'^ANTARA\\s*-\\s*', '', text, flags=re.IGNORECASE)\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:()-]', '', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def clean_articles(self, articles: List[Dict]) -> List[Dict]:\n",
    "        print(f\"Cleaning {len(articles)} articles...\")\n",
    "        \n",
    "        for article in articles:\n",
    "            if 'content' in article:\n",
    "                article['content'] = self.clean_text(article['content'])\n",
    "            article['title'] = self.clean_text(article['title'])\n",
    "            article['summary'] = self.clean_text(article['summary'])\n",
    "        \n",
    "        print(\"Text cleaning completed!\")\n",
    "        return articles\n",
    "\n",
    "# Cell 6: Negative News Detection Agent\n",
    "class NegativeNewsDetectionAgent:\n",
    "    def __init__(self, keywords: List[str]):\n",
    "        self.keywords = [kw.lower() for kw in keywords]\n",
    "        self.keyword_scores = {}\n",
    "    \n",
    "    def detect_negative_keywords(self, text: str) -> tuple:\n",
    "        text_lower = text.lower()\n",
    "        matched_keywords = []\n",
    "        \n",
    "        for keyword in self.keywords:\n",
    "            count = text_lower.count(keyword)\n",
    "            if count > 0:\n",
    "                matched_keywords.extend([keyword] * count)\n",
    "        \n",
    "        keyword_score = len(matched_keywords)\n",
    "        return keyword_score, matched_keywords\n",
    "    \n",
    "    def is_negative(self, keyword_score: int) -> bool:\n",
    "        return keyword_score >= 2\n",
    "    \n",
    "    def analyze_article(self, article: Dict) -> Dict:\n",
    "        full_text = ' '.join([\n",
    "            article.get('title', ''),\n",
    "            article.get('summary', ''),\n",
    "            article.get('content', '')\n",
    "        ])\n",
    "        \n",
    "        keyword_score, matched_keywords = self.detect_negative_keywords(full_text)\n",
    "        is_neg = self.is_negative(keyword_score)\n",
    "        \n",
    "        article['negative_score'] = keyword_score\n",
    "        article['is_negative'] = is_neg\n",
    "        article['matched_keywords'] = matched_keywords\n",
    "        article['processed_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        return article\n",
    "    \n",
    "    def analyze_articles(self, articles: List[Dict]) -> List[Dict]:\n",
    "        print(f\"Analyzing {len(articles)} articles for negative content...\")\n",
    "        \n",
    "        analyzed_articles = []\n",
    "        negative_count = 0\n",
    "        \n",
    "        for article in articles:\n",
    "            analyzed = self.analyze_article(article)\n",
    "            analyzed_articles.append(analyzed)\n",
    "            if analyzed['is_negative']:\n",
    "                negative_count += 1\n",
    "        \n",
    "        print(f\"Analysis complete: {negative_count}/{len(articles)} articles marked as negative\")\n",
    "        return analyzed_articles\n",
    "\n",
    "# Cell 7: Storage & Logging Agents\n",
    "class StorageAgent:\n",
    "    def __init__(self, data_dir: str):\n",
    "        self.data_dir = data_dir\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    def save_articles(self, articles: List[Dict], filename: str = None) -> str:\n",
    "        if filename is None:\n",
    "            filename = f\"{TODAY}_antaranews.json\"\n",
    "        \n",
    "        filepath = os.path.join(self.data_dir, filename)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Saved {len(articles)} articles to: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "\n",
    "class LoggingAgent:\n",
    "    def __init__(self, logs_dir: str):\n",
    "        self.logs_dir = logs_dir\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        self.log_file = os.path.join(logs_dir, f\"{TODAY}_errors.log\")\n",
    "    \n",
    "    def log_error(self, url: str, error_message: str):\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        log_entry = f\"{timestamp} | {url} | {error_message}\\n\"\n",
    "        \n",
    "        with open(self.log_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(log_entry)\n",
    "    \n",
    "    def log_failed_urls(self, failed_urls: List[tuple]):\n",
    "        if not failed_urls:\n",
    "            print(\"No errors to log.\")\n",
    "            return\n",
    "        \n",
    "        for url, error in failed_urls:\n",
    "            self.log_error(url, error)\n",
    "        \n",
    "        print(f\"Logged {len(failed_urls)} errors to: {self.log_file}\")\n",
    "\n",
    "# Cell 8: Run Complete Pipeline\n",
    "def run_pipeline():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"INDONESIA LOCAL NEWS NEGATIVE EVENT DETECTION SYSTEM\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Pipeline started at: {datetime.now().isoformat()}\\n\")\n",
    "    \n",
    "    # Step 1: RSS Ingestion\n",
    "    print(\"STEP 1: RSS Ingestion\")\n",
    "    print(\"-\" * 40)\n",
    "    rss_agent = RSSIngestionAgent(RSS_URL)\n",
    "    all_articles = rss_agent.fetch_feed()\n",
    "    today_articles = rss_agent.filter_today_articles(all_articles)\n",
    "    \n",
    "    if not today_articles:\n",
    "        print(\"No articles from today found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Article Scraping\n",
    "    print(\"\\nSTEP 2: Article Scraping\")\n",
    "    print(\"-\" * 40)\n",
    "    scraper_agent = ArticleScraperAgent(delay=REQUEST_DELAY, max_retries=MAX_RETRIES)\n",
    "    enriched_articles = scraper_agent.scrape_articles(today_articles)\n",
    "    \n",
    "    if not enriched_articles:\n",
    "        print(\"No articles could be scraped. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Text Cleaning\n",
    "    print(\"\\nSTEP 3: Text Cleaning\")\n",
    "    print(\"-\" * 40)\n",
    "    cleaning_agent = TextCleaningAgent()\n",
    "    cleaned_articles = cleaning_agent.clean_articles(enriched_articles)\n",
    "    \n",
    "    # Step 4: Negative News Detection\n",
    "    print(\"\\nSTEP 4: Negative News Detection\")\n",
    "    print(\"-\" * 40)\n",
    "    detection_agent = NegativeNewsDetectionAgent(ALL_NEGATIVE_KEYWORDS)\n",
    "    analyzed_articles = detection_agent.analyze_articles(cleaned_articles)\n",
    "    \n",
    "    # Step 5: Storage\n",
    "    print(\"\\nSTEP 5: Storage\")\n",
    "    print(\"-\" * 40)\n",
    "    storage_agent = StorageAgent(DATA_DIR)\n",
    "    output_file = storage_agent.save_articles(analyzed_articles)\n",
    "    \n",
    "    # Step 6: Logging\n",
    "    print(\"\\nSTEP 6: Error Logging\")\n",
    "    print(\"-\" * 40)\n",
    "    logging_agent = LoggingAgent(LOGS_DIR)\n",
    "    logging_agent.log_failed_urls(scraper_agent.failed_urls)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE COMPLETED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total articles processed: {len(analyzed_articles)}\")\n",
    "    negative_count = sum(1 for a in analyzed_articles if a['is_negative'])\n",
    "    print(f\"Negative articles detected: {negative_count}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"Completed at: {datetime.now().isoformat()}\")\n",
    "    \n",
    "    return analyzed_articles\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_pipeline()\n",
    "    \n",
    "    # Display summary\n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ARTICLE SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        negative_articles = [a for a in results if a['is_negative']]\n",
    "        if negative_articles:\n",
    "            print(f\"\\nNEGATIVE ARTICLES ({len(negative_articles)}):\\n\")\n",
    "            for article in negative_articles:\n",
    "                print(f\"Title: {article['title'][:70]}...\")\n",
    "                print(f\"  Score: {article['negative_score']} | URL: {article['url'][:60]}...\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"\\nNo negative articles detected today.\")\n",
    "        \n",
    "        print(\"\\nSTATISTICS\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Total Articles: {len(results)}\")\n",
    "        print(f\"Negative Articles: {len(negative_articles)}\")\n",
    "        if results:\n",
    "            print(f\"Negative Percentage: {len(negative_articles)/len(results)*100:.1f}%\")\n",
    "            avg_score = sum(a['negative_score'] for a in results) / len(results)\n",
    "            print(f\"Avg Negative Score: {avg_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
